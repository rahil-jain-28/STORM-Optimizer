{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STORM optimizer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7Dflz3ccLdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim import Optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY1cofGOb0qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class STORM(Optimizer):\n",
        "    r\"\"\"Implements STORM algorithm.\n",
        "\n",
        "    It has been proposed in `Momentum-Based Variance Reduction in Non-Convex SGD`_.\n",
        "    ... Momentum-Based Variance Reduction in Non-Convex SGD:\n",
        "        https://arxiv.org/abs/1905.10018\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        k (float, optional): hyperparameter as described in paper\n",
        "        w (float, optional): hyperparameter as described in paper\n",
        "        c (float, optional): hyperparameter to be swept over logarithmically spaced grid as per paper\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, k=0.1, w=0.1, c=1, weight_decay=0):\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(k=k, w=w, c=c, weight_decay=weight_decay)\n",
        "        super(STORM, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(STORM, self).__setstate__(state)\n",
        "\n",
        "    # Performs a single optimization step \n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('STORM does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    # No. of steps\n",
        "                    state['step'] = 0\n",
        "                    # Learning rate (given as 'η(t)' in paper for step t)\n",
        "                    state['lr'] = group['k']/group['w']**(1/3)\n",
        "                    # Square of gradients (given as 'G(t)^2' in paper for step t)\n",
        "                    state['G^2'] = 0\n",
        "                    # Momentum (given as 'd(t)' in paper for step t)\n",
        "                    state['d'] = 0\n",
        "                    # Gradients before optimization step (given as G(t-1) in paper for step t)\n",
        "                    state['prev_grad'] = 0\n",
        "                    # Given as 'c.η(t)^2' in paper for step t\n",
        "                    state['a'] = 0\n",
        "                \n",
        "                # Retrieving variables\n",
        "                grad_sqr_sum, d, learning_rate,prev_grad,a = state['G^2'], state['d'], state['lr'], state['prev_grad'],state['a']\n",
        "                k, w, c = group['k'], group['w'], group['c']\n",
        "                state['step'] += 1\n",
        "\n",
        "                # weight decay\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(p, alpha=group['weight_decay'])\n",
        "                \n",
        "                # Change in state for this optimization step\n",
        "                grad_sqr_sum += (torch.norm(grad).item())**2\n",
        "                learning_rate = k/(w + grad_sqr_sum)**(1/3)\n",
        "                d = grad + (1-a)*(d-prev_grad)\n",
        "\n",
        "                # Data update step\n",
        "                p.data = p.data - learning_rate*d\n",
        "                \n",
        "                # Change in state for next optimization step\n",
        "                a = c*(learning_rate**2)\n",
        "                prev_grad = grad\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNpPGpCS30ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}